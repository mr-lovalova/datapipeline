from __future__ import annotations

from collections.abc import Iterator
from typing import Literal

from datapipeline.domain.sample import Sample
from datapipeline.domain.vector import Vector
from datapipeline.services.artifacts import (
    ArtifactNotRegisteredError,
    VECTOR_METADATA_SPEC,
)

from .common import ContextExpectedMixin, replace_vector, select_vector, try_get_current_context


class VectorDropPartitionsTransform(ContextExpectedMixin):
    """Drop partitions/features when metadata coverage falls below configured thresholds.

    Requires the optional `schema.metadata.json` artifact generated by the
    `metadata` build task. The transform evaluates coverage using the recorded
    `present_count`/`null_count` metrics and prunes the schema cache once so
    downstream coverage checks stop expecting bad partitions.
    """

    def __init__(
        self,
        *,
        payload: Literal["features", "targets"] = "features",
        min_coverage: float | None = None,
        min_value_fraction: float | None = None,
    ) -> None:
        super().__init__(payload=payload)
        if min_coverage is None and min_value_fraction is None:
            raise ValueError("VectorDropPartitionsTransform requires min_coverage and/or min_value_fraction.")
        if min_coverage is not None and not 0.0 <= min_coverage <= 1.0:
            raise ValueError("min_coverage must be between 0 and 1.")
        if min_value_fraction is not None and not 0.0 <= min_value_fraction <= 1.0:
            raise ValueError("min_value_fraction must be between 0 and 1.")
        self._min_coverage = min_coverage
        self._min_value_fraction = min_value_fraction
        self._drop_ids: set[str] | None = None
        self._schema_pruned = False

    def __call__(self, stream: Iterator[Sample]) -> Iterator[Sample]:
        return self.apply(stream)

    def apply(self, stream: Iterator[Sample]) -> Iterator[Sample]:
        drop_ids = self._resolve_drop_ids()
        if not drop_ids:
            yield from stream
            return
        self._maybe_prune_schema(drop_ids)
        for sample in stream:
            vector = select_vector(sample, self._payload)
            if vector is None or not vector.values:
                yield sample
                continue
            retained: dict[str, object] = {}
            changed = False
            for fid, value in vector.values.items():
                if fid in drop_ids:
                    changed = True
                    continue
                retained[fid] = value
            if not changed:
                yield sample
            else:
                yield replace_vector(sample, self._payload, Vector(values=retained))

    def _resolve_drop_ids(self) -> set[str]:
        if self._drop_ids is not None:
            return self._drop_ids
        context = self._context or try_get_current_context()
        if not context:
            raise RuntimeError("VectorDropPartitionsTransform requires an active pipeline context.")
        try:
            metadata = context.require_artifact(VECTOR_METADATA_SPEC)
        except ArtifactNotRegisteredError as exc:
            raise RuntimeError(
                "Vector metadata artifact missing. Enable the `metadata` build task "
                "and rerun `jerry build --project <project.yaml>`."
            ) from exc
        section_key = "targets" if self._payload == "targets" else "features"
        counts_key = "target_vectors" if self._payload == "targets" else "feature_vectors"
        entries = metadata.get(section_key) or []
        total = metadata.get("counts", {}).get(counts_key)
        if not isinstance(total, (int, float)) or total <= 0:
            if self._payload == "targets":
                raise RuntimeError(
                    "Vector metadata artifact missing counts for targets; "
                    "set `include_targets: true` in build/artifacts/metadata.yaml and rebuild."
                )
            raise RuntimeError(
                "Vector metadata artifact missing counts for features; "
                "rerun `jerry build --project <project.yaml>` to refresh metadata."
            )
        drop_ids: set[str] = set()
        for entry in entries:
            if not isinstance(entry, dict):
                continue
            fid = entry.get("id")
            if not isinstance(fid, str):
                continue
            present = float(entry.get("present_count") or 0)
            nulls = float(entry.get("null_count") or 0)
            coverage = present / float(total) if total else 0.0
            value_fraction = (present - nulls) / present if present > 0 else 0.0
            if self._min_coverage is not None and coverage < self._min_coverage:
                drop_ids.add(fid)
                continue
            if self._min_value_fraction is not None and value_fraction < self._min_value_fraction:
                drop_ids.add(fid)
        self._drop_ids = drop_ids
        return drop_ids

    def _maybe_prune_schema(self, drop_ids: set[str]) -> None:
        if self._schema_pruned or not drop_ids:
            return
        context = self._context or try_get_current_context()
        if not context:
            self._schema_pruned = True
            return
        cache = getattr(context, "_cache", None)
        if cache is None:
            self._schema_pruned = True
            return
        schema_key = f"schema:{self._payload}"
        if schema_key not in cache:
            context.load_schema(payload=self._payload)
        entries = cache.get(schema_key)
        if not entries:
            self._schema_pruned = True
            return
        kept = [entry for entry in entries if entry.get("id") not in drop_ids]
        cache[schema_key] = kept
        ids_key = f"expected_ids:{self._payload}"
        cache[ids_key] = [
            entry.get("id")
            for entry in kept
            if isinstance(entry.get("id"), str)
        ]
        self._schema_pruned = True
